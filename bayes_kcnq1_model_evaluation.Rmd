---
title: "Bayes KCNQ1 Model Evaluation"
author: "Matthew O'Neill"
date: "7/6/2022"
output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: false
      # collapsed: true
    smooth_scroll: true
    code_folding: hide
    highlight: zenburn #textmate
    theme: flatly
    # number_sections: true
editor_options: 
  chunk_output_type: console
---


```{r preamble,include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library("nnet")
library("DBI")
library("RSQLite")
library(dplyr)
library(ggplot2)
library(ggpubr)
library(caret)
library(plotrix)
library(glmnet)
library(meta)
library(reshape2)
library(psych)
require(Hmisc)
library(tableone)
library(wCorr)
library(rms)
library(boot)
library(leaps)
library(car)
library(reticulate)
library(rootSolve)
library(pROC)
library(wCorr)
library(MALDIquant)
library(tidyverse)      
library(lubridate)     
library(fpp2)          
library(zoo)            
library(latex2exp)
library(forestplot)
library(ggplot2)
library(caret)


source('RMD and Scripts/func_dist_seq.R')

# Dataset from Bayes Processing RMD
load('RMD and Scripts/KCNQ1_prepared_data') 

# Include distances between residue centroids from structure of KCNQ1 PDB-ID: 6UZZ
q1dist <- read.csv(file = 'Covariates/kcnq1_distances.csv', header = FALSE)

```

# Introduction

This document describes our Bayesian penetrance estimation protocol for the KCNQ1-LQT1 genotype-phenotype relationship. We build the model based on observed heterozygote phenotypes and variant specific features, including function, structure, and in silico predictions. We evaluate our method using Spearman and Pearson correlations, Brier Scores, and 10-fold cross validation. This work builds on previously published work for SCN5A (PMID: 32569262) and KCNH2 (PMID: 34309407). 

Variant-specific data and clinical data are curated in the Bayes KCNQ1 Data Processing RMD. The current document withholds the LQT1-dist metric for each variant being evaluated. For prospective applications, all LQT1-dist information is incorporated. 

# Model Construction
Here we construct the model. Additional details are provided in the Supplemental Methods. 

## Literature and Cohort Combined

```{r}

# Literature dataset where potentially overlapping carriers/heterozygotes are removed

d <- lit.cohort.data[lit.cohort.data$mut_type == "missense",]

# set initial weighting and penetrance - see materials and methods 

d$weight = 1-1/(0.01+d$total_carriers)
d$penetrance_lqt1 <- d$lqt1/d$total_carriers
d[d$total_carriers < 1,"weight"] <- 0.000 

```


## LQT1 empirical diagnosis probability prior
Use observed LQT1 diagnosis probability to calculate "LQTS probability density" as described in previous publication. Plot diagnosis probability density versus residue

```{r}

# Mean squared error

mse <- function(sm) {
  mean((sm$residuals)^2*(sm$weights))
}

# Derive alpha and beta from weighted mean and MSE (estimated variance)

estBetaParams <- function(mu, var) {
  alpha <- ((1 - mu) / var - 1 / mu) * mu ^ 2
  beta <- alpha * (1 / mu - 1)
  return(params = list(alpha = alpha, beta = beta))
}

# Weighted mean to determine lqt1 penetrance empirical prior

newdata = data.frame(wt=1)
model <- lm(penetrance_lqt1 ~ 1, data=d, weights = d$weight)
summary(model)
p<-predict(model, newdata)
dev<- mse(model) #p*(1-p)

# Estimated shape parameters for lqt1 empirical prior

alpha0 = estBetaParams(p,dev)$alpha
beta0 = estBetaParams(p,dev)$beta
print(paste("alpha0 = ", alpha0, "  beta0 = ", beta0))

# Bayesian lqt1 penetrance estimates from empirical priors 
# and observed affected/unaffected counts:

d$lqt1_penetranceBayesian_initial <- (alpha0 + d[,"lqt1"])/((alpha0 + beta0 + d[,"total_carriers"]))
d$lqt1_penetranceBayesian<-d$lqt1_penetranceBayesian_initial

m<- d %>% 
  select(resnum, pmean = penetrance_lqt1) %>% 
  mutate(p_mean_smooth = rollmean(pmean, k=20, fill = NA))

# Penetrance estimate by Residue - Supplemental Figure 2 

fit <- loess(d[,"penetrance_lqt1"]~as.numeric(d[,"resnum"]), span = 0.15)
plot(d$resnum, d$penetrance_lqt1, xlab ="Residue", ylab = "LQT1 Penetrance Estimate")
xrange <- seq(min(fit$x), max(fit$x), length.out = 100)
ps <- predict(fit, xrange, se=T)
lines(xrange, ps$fit*1, lwd=5)
lines(xrange, (ps$fit+1.96*ps$se.fit)*1, lty=2, lwd=4)
lines(xrange, (ps$fit-1.96*ps$se.fit)*1, lty=2, lwd=4)

```

## Calculate LQTS probability densities and annotate function and structural location
With the updated empirical priors applied to carrier counts, calculate "LQTS probability density" as described in previous publication.

```{r, echo=T, results='hide'}

# MODEL 1 - this method for model evaluations - Spearman, 10-fold, and Briers 

q1.covariates <- q1.covariates[!is.na(q1.covariates$resnum), ]
q1.covariates[, "lqt1_dist"]<-NA
q1.covariates[, "lqt1_dist_weight"]<-NA
ld<-0
for(rec in 1:nrow(q1.covariates)){
print(rec)
q1.covariates[rec,c("lqt1_dist", "lqt1_dist_weight")] <- funcdist(q1.covariates[rec, "resnum"], q1.covariates[rec, "var"], d[!is.na(d$total_carriers) & d$total_carriers>0 & d$mut_type != "nonsense",], q1dist, "penetrance_lqt1", "sigmoid", 7)
}

```

```{r, echo=T}

# Plot lqt1_dist versus residue number

q1.covariates$resnum<-as.integer(q1.covariates$resnum)
q1.covariates <- q1.covariates[order(q1.covariates$resnum),]
q1.covariates <- q1.covariates[!is.na(q1.covariates$resnum),]
m<- q1.covariates %>% 
  select(resnum, pmean = lqt1_dist) %>% 
  mutate(p_mean_smooth = rollmean(pmean, k=20, fill = NA))


# Plot penetrance density by residue - Supplemental Figure 2

fit <- loess(q1.covariates[,"lqt1_dist"]~as.numeric(q1.covariates[,"resnum"]), span = 0.15)
plot(q1.covariates$resnum, q1.covariates$lqt1_dist, xlab ="Residue", ylab = "LQT1 Penetrance Density", xlim=c(0,680))
xrange <- seq(min(fit$x), max(fit$x), length.out = 100)
ps <- predict(fit, xrange, se=T)
lines(xrange, ps$fit*1, lwd=5)
lines(xrange, (ps$fit+1.96*ps$se.fit)*1, lty=2, lwd=4)
lines(xrange, (ps$fit-1.96*ps$se.fit)*1, lty=2, lwd=4)

```

## Merge Clinical Dataframe with Covariate Dataframe

```{r}

q1.covariates <- distinct(q1.covariates, var, .keep_all = TRUE)
q1.covariates[is.na(q1.covariates$mut_type), "mut_type"] <- "missense"
d$resnum <- as.integer(d$resnum)
d <- merge(d, q1.covariates, all = TRUE)
d[is.na(d$total_carriers), "total_carriers"] <- 0
d <- unique(d)

```

## Calculate Bayesian priors and posteriors for all variants

```{r, echo=T, results='hide'}

# Assign p_mean_w to empirical penetrance. The reassignment to ensures
# p_mean_w does not contain "NA"s.

d[is.na(d$lqt1_penetranceBayesian),"lqt1_penetranceBayesian"] <- alpha0/(alpha0+beta0)
d$p_mean_w <- d$lqt1_penetranceBayesian

# reassign "NA"s to 0 in heterozygote counts to enable updating during EM iterations

d[is.na(d$total_carriers),"total_carriers"] <- 0 
d[is.na(d$lqt1),"lqt1"] <- 0 
d[is.na(d$unaff),"unaff"] <- 0 

regression <- function(dv, pivs, nivs, data) {
  # run a linear model with text arguments for dv and ivs
  piv_string <- paste(pivs, collapse=" + ")
  niv_string <- paste(nivs, collapse=" - ")
  if(niv_string!="") iv_string <- paste(piv_string, " - ", niv_string, sep = "")
  if(niv_string=="") iv_string <- paste(piv_string)
  #print(iv_string)
  regression_formula <- as.formula(paste(dv, iv_string, sep=" ~ "))
  #print(regression_formula)
  glm(regression_formula, data, family = quasibinomial(link = "logit"), weights = data[,"weight"])
}

# solve for alpha and beta in Beta distribution

solab <- function(mean, variance){
  alpha <- (mean^2 * (1-mean) - variance * mean)/variance
  beta <- alpha * (1 / mean - 1)
  return(c(alpha,beta))
}

# These 5 features included in the EM algorithm 
covariates <- c("blast_pssm", "revel_score", "cardiacboost", "hm_peak", "lqt1_dist")
delta <- 10
count <- 0
tmp<-d[!is.na(d$mut_type) & d$mut_type=="missense" & !is.na(d$lqt1_dist),]
while(delta > 1 & count < 25){ # 5 is roughly a change of 0.5%
  print(paste(delta, count))
  count <- count + 1
  alpha_f <- NULL
  beta_f <- NULL
  
  for(i in 1:nrow(tmp)){
  newdata = data.frame(var=tmp[i,"var"])
  newdata[covariates] <- tmp[i,covariates]
  model <- regression("p_mean_w", covariates, 
                      colnames(newdata)[colSums(is.na(newdata))>0], tmp)
  mean_f <- predict(model, newdata, type = "response")
  variance_f <- (predict(model, newdata,se.fit = T, type = "response")$se.fit)^2
  alpha <- solab(mean_f,variance_f)[1]
  beta <- solab(mean_f,variance_f)[2]
  tmp[i,"prior_mean_w"] <- mean_f
  if(alpha<0.01 | beta<0.01){
    alpha_f[i]=alpha0
    beta_f[i]=beta0
  }else{
    alpha_f[i]=alpha
    beta_f[i]=beta
  }
  }
  new_mean <- (alpha_f + tmp$lqt1)/(alpha_f + beta_f + tmp$total_carriers)
  delta <- sum(abs(new_mean-tmp$p_mean_w))
  tmp$p_mean_w <- new_mean
  print(delta)
}

for (variant in tmp$var){t<-NA;t<-tmp[tmp$var == variant, c("prior_mean_w", "p_mean_w")]; d[d$var == variant, c("prior_mean_w", "p_mean_w")] <- t[1,] }

for (variant in mut_type$var){print(variant);
  if (!is.na(match(variant,d$var)) & !is.na(match(variant, lit.cohort.data$var))) {
    d[d$var == variant, c("lqt1", "unaff","total_carriers")]<-lit.cohort.data[lit.cohort.data$var == variant, c("lqt1", "unaff","total_carriers")] 
    }
}

# when tuning parameter is 11, predictions are equivalent to 10 variant heterozygote phenotypes 
prior_mean <- d$p_mean_w
variance <- prior_mean*(1-prior_mean)
variance <- variance / 11
ind_a <- seq(1, length(variance),1)
ind_b <- seq(length(variance)+1, length(variance)*2,1)
alpha <- solab(prior_mean,variance)[ind_a]
beta <- solab(prior_mean,variance)[ind_b]

new_mean <- (alpha + d$lqt1)/(alpha + beta + d$total_carriers)
d$p_mean_w <- new_mean
d$prior_mean <- (alpha)/(alpha + beta)
d[d$total_carriers<1,"p_mean_w"]<-d[d$total_carriers<1,"prior_mean_w"]

d$alpha <- alpha
d$beta <- beta

# Save literature data for all variants

save(d,file = "lit_plus_cohort_checkpoint_eval.RData")


```

# Calculate correlations of covariates with empirical posteriors
We next evaluate our model using Spearman correlations, Pearson correlations, and Brier scores (see below). 

## Spearman Lit and Cohort merged 

```{r}

# Data for text and Supplemental Table 1 
calcPval=function(xName,yName,weightName,nPerms,new.mat2){
  # Pulls out variables
  x=new.mat2[,xName] 
  y=new.mat2[,yName] 
  w=new.mat2[,weightName]
  x2=x[!is.na(x)]
  y2=y[!is.na(x)]
  w2=w[!is.na(x)]

  # Calculate the real correlation
  realCorr=weightedCorr(x2,y2,method='spearman',weights=w2)
  # Do permutations, calculate fake correlations
  permutedCorrList=c()
  for(permNum in 1:nPerms){
    permutedX=sample(x2,length(x2),replace=FALSE)
    wCorrSim=weightedCorr(permutedX,y2,method='spearman',weights=w2)
    permutedCorrList=c(permutedCorrList,wCorrSim)
  }
  permutedCorrList2=abs(permutedCorrList)
  realCorr2=abs(realCorr)
  
  # Calculate pvalue
  summ=sum(realCorr2<permutedCorrList2)
  pValue=summ/nPerms
  return(list(realCorr,pValue,length(x2)))
}

calcAllPvals=function(yList,xList,nPerms,weightName,new.mat2){
  i=0
  resultTable=data.frame()
  for(yName in yList){
    for(xName in xList){
      i=i+1
      result=calcPval(xName,yName,weightName,nPerms,new.mat2)
      resultTable[i,'x']=xName
      resultTable[i,'y']=yName
      resultTable[i,'nPerms']=nPerms
      resultTable[i,'weightedCorr']=result[[1]]
      resultTable[i,'pValue']=result[[2]]
      resultTable[i,'n']=result[[3]]
      print(resultTable[i,'pValue'])
    }
  }
  print(resultTable)
  return(resultTable)
}

yList=c('lqt1_penetranceBayesian')
xList=c("lqt1_dist", "cardiacboost", 
        'hm_peak','hm_Vhalfact','hm_tauact','hm_taudeact',
        'ht_peak','ht_Vhalfact','ht_tauact','ht_taudeact',
        'pph2_prob', 'provean_score', 'revel_score', "blast_pssm", "prior_mean", "p_mean_w"
        ) 
tmp<-d[!is.na(d$penetrance_lqt1) & !is.na(d$revel_score),] 

resultTable<-calcAllPvals(yList, xList, 1000, 'weight', tmp)


```

## Pearson Lit and Cohort Merged 

```{r}

# Data for text and Supplemental Table 1 

yList=c('lqt1_penetranceBayesian')
xList=c("lqt1_dist", "cardiacboost", 
        'hm_peak','hm_Vhalfact','hm_tauact','hm_taudeact',
        'ht_peak','ht_Vhalfact','ht_tauact','ht_taudeact',
        'pph2_prob', 'provean_score', 'revel_score', "blast_pssm", "prior_mean", "p_mean_w"
        ) 
tmp<-d[!is.na(d$penetrance_lqt1) & !is.na(d$revel_score),] 
resultTable<-calcAllPvals(yList, xList, 1000, 'weight', tmp)

```


## Spearman Forest Plots for All Variant Heterozygotes

```{r}

d2 <- d[!is.na(d$total_carriers) & d$mut_type == "missense",]
tmp<-d2[!is.na(d2$provean_score) & !is.na(d2$revel_score),] 
yList=c('lqt1_penetranceBayesian')
xList=c( 
        'hm_peak','hm_Vhalfact','hm_tauact','hm_taudeact',
        'ht_peak','ht_Vhalfact','ht_tauact','ht_taudeact',
        "cardiacboost", 'pph2_prob', 'revel_score', 'provean_score', 
        "blast_pssm", "lqt1_dist", 'prior_mean_w', 'p_mean_w')
# Heterozygous data is empty in this set! Find out what happened
resultTable<-calcAllPvals(yList, xList, 1000, 'weight', tmp)

rm(tmp)
rm(t)
i=0
tmp<-data.frame()
for (x in xList){
  i=i+2
  tmp[i-1,"Feature"]<-x
  t<-d[!is.na(d[,x]) & d$total_carriers>0,]
  t<-t[!is.na(t[,"var"]),]
  foo <- boot(t, function(data,indices)
  weightedCorr(t[indices,x],t$penetrance_lqt1[indices], method="spearman", weights = t$weight[indices]), R=1000)
  tmp[i-1,"Spearman"]<-foo$t0
  tmp[i-1,"Spearman_low"]<-quantile(foo$t,c(0.025,0.975), na.rm = T)[1][[1]] 
  tmp[i-1,"Spearman_high"]<-quantile(foo$t,c(0.025,0.975), na.rm = T)[2][[1]]
  tmp[i-1,"n"]<-length(t[,x])
  
}

# Data for Figure 2 
forestplot(tmp$Feature,tmp$Spearman,tmp$Spearman_low,tmp$Spearman_high)


```


# 10-fold Cross Validation for Spearman, Pearson, and Brier scores 
We perform 10-fold cross validation to assess the optimism of our estimates across the dataset. We do this using 3 different metrics. 

## Import Data 

```{r}

# overlap of variants - remove non-uniques
d <- distinct(d, var, .keep_all = TRUE)
d <- d[d$total_carriers >0 , ]

# index rows randomly - need to randomize rows first in the final 
d$ID <- seq.int(nrow(d))
d <- d[sample(nrow(d)),]

# set prior and posterior to 0 when using saved or loaded data
d$prior_mean_w <- NULL
d$p_mean_w <- NULL
d$alpha <- NULL
d$beta <- NULL

# Only test on data for which we have carriers
d <- d[d$total_carriers > 0, ]

# Turn off warnings to speed computation
options(warn = -1)

```


## Functions for 10-fold CV

```{r}

regression <- function(dv, pivs, nivs, data) {
  # run a linear model with text arguments for dv and ivs - positive input variable, negative input variable 
  piv_string <- paste(pivs, collapse=" + ")
  niv_string <- paste(nivs, collapse=" - ")
  if(niv_string!="") iv_string <- paste(piv_string, " - ", niv_string, sep = "")
  if(niv_string=="") iv_string <- paste(piv_string)
  #print(iv_string)
  regression_formula <- as.formula(paste(dv, iv_string, sep=" ~ "))
  #print(regression_formula)
  glm(regression_formula, data, family = quasibinomial(link = "logit"), weights = data[,"weight"])
}

# solve for alpha and beta in Beta distribution
solab <- function(mean, variance){
  alpha <- (mean^2 * (1-mean) - variance * mean)/variance
  beta <- alpha * (1 / mean - 1)
  return(c(alpha,beta))
}

# calculate Brier score
brier_calc <- function(covariate, outcome, count){
  score <- sum((covariate - outcome)^2)/count
  print(score)
}

```


## 10-fold CV Spearman Cor - Implementation

```{r, echo=T, results='hide'}

# Caret package to create 10 folds 
d2 <- d
k = 10
kfold = k
d3 <- createFolds(d2$ID, k=kfold)


# Make a vector to host Spearman correlations for ith fold 
spearman_cor <- vector("numeric", 10)
j = 0

# Working loop to set ith fold to weight 0! 
for(i in d3){
  j = j + 1
  
# Part 1 - split data
  test <- d2[i,]
  test$weight <- 0
  train <- d2[-i,]
  full <- rbind(test, train) # 10 dataframes where ith variants have weight = 0 

# Part 2 - run EM algorithm
full$p_mean_w <- full$penetrance_lqt1
covariates <- c("blast_pssm", "revel_score", "cardiacboost", "hm_peak", "lqt1_dist")
delta <- 10
count <- 0
tmp <- full
options(warn = -1)

while(delta > 1 & count < 25){ # delta = 5 is roughly a change of 0.5%
  print(paste(delta, count))
  count <- count + 1
  alpha_f <- NULL
  beta_f <- NULL
  
  for(i in 1:nrow(tmp)){
  newdata = data.frame(var=tmp[i,"var"])
  newdata[covariates] <- tmp[i,covariates]
  model <- regression("p_mean_w", covariates, 
                      colnames(newdata)[colSums(is.na(newdata))>0], tmp)
  mean_f <- predict(model, newdata, type = "response")
  variance_f <- (predict(model, newdata,se.fit = T, type = "response")$se.fit)^2
  alpha <- solab(mean_f,variance_f)[1]
  beta <- solab(mean_f,variance_f)[2]
  tmp[i,"prior_mean_w"] <- mean_f
  if(alpha<0.01 | beta<0.01){
    alpha_f[i]=alpha0
    beta_f[i]=beta0
  }else{
    alpha_f[i]=alpha
    beta_f[i]=beta
  }
  }
  new_mean <- (alpha_f + tmp$lqt1)/(alpha_f + beta_f + tmp$total_carriers)
  full$prior_mean_w <- tmp$prior_mean_w
  delta <- sum(abs(new_mean-tmp$p_mean_w))
  tmp$p_mean_w <- new_mean
  full$p_mean_w <- tmp$p_mean_w
  print(delta)
}

full2 <- full[full$weight == 0, ]

# Crossfold Validation reported in text and Table 1 

spearman_cor[j] <- weightedCorr(full2$lqt1_penetranceBayesian,full2$prior_mean_w,method='spearman',weights= (1-1/(0.01+full2$total_carriers)))
print(spearman_cor)

}


```

## 10-fold CV Pearson Cor - Implementation

```{r, echo=T, results='hide'}

# Caret package to create 10 folds 

d2 <- d
k = 10
kfold = k
d3 <- createFolds(d2$ID, k=kfold)


# Make a vector to host Spearman correlations for ith fold 

pearson_cor <- vector("numeric", 10)
j = 0

# Working loop to set ith fold to weight 0! 
for(i in d3){
  j = j + 1
  
# Part 1 - split data
  test <- d2[i,]
  test$weight <- 0
  train <- d2[-i,]
  full <- rbind(test, train) # 10 dataframes where ith variants have weight = 0 

# Part 2 - run EM algorithm
full$p_mean_w <- full$penetrance_lqt1
covariates <- c("blast_pssm", "revel_score", "cardiacboost", "hm_peak", "lqt1_dist")
delta <- 10
count <- 0
tmp <- full
options(warn = -1)

while(delta > 1 & count < 25){ # delta = 5 is roughly a change of 0.5%
  print(paste(delta, count))
  count <- count + 1
  alpha_f <- NULL
  beta_f <- NULL
  
  for(i in 1:nrow(tmp)){
  newdata = data.frame(var=tmp[i,"var"])
  newdata[covariates] <- tmp[i,covariates]
  model <- regression("p_mean_w", covariates, 
                      colnames(newdata)[colSums(is.na(newdata))>0], tmp)
  mean_f <- predict(model, newdata, type = "response")
  variance_f <- (predict(model, newdata,se.fit = T, type = "response")$se.fit)^2
  alpha <- solab(mean_f,variance_f)[1]
  beta <- solab(mean_f,variance_f)[2]
  tmp[i,"prior_mean_w"] <- mean_f
  if(alpha<0.01 | beta<0.01){
    alpha_f[i]=alpha0
    beta_f[i]=beta0
  }else{
    alpha_f[i]=alpha
    beta_f[i]=beta
  }
  }
  new_mean <- (alpha_f + tmp$lqt1)/(alpha_f + beta_f + tmp$total_carriers)
  full$prior_mean_w <- tmp$prior_mean_w
  delta <- sum(abs(new_mean-tmp$p_mean_w))
  tmp$p_mean_w <- new_mean
  full$p_mean_w <- tmp$p_mean_w
  print(delta)
}

full2 <- full[full$weight == 0, ]

pearson_cor[j] <- weightedCorr(full2$lqt1_penetranceBayesian,full2$prior_mean_w,method='pearson',weights= (1-1/(0.01+full2$total_carriers)))
print(pearson_cor)

}


```

## 10-fold CV Brier Score - Implementation

```{r, echo=T, results='hide'}

# Caret package to create 10 folds 
d2 <- d
k = 10
kfold = k
d3 <- createFolds(d2$ID, k=kfold)

# Make a vector to host Brier scores for the ith fold 
brier_scores <- vector("numeric", 10)
j = 0

# Working loop to set ith fold to weight 0! 
for(i in d3){
  j = j + 1
  
# Part 1 - split data
  test <- d2[i,]
  test$weight <- 0
  train <- d2[-i,]
  full <- rbind(test, train) # 10 dataframes where ith variants have weight = 0 

# Part 2 - run EM algorithm
full$p_mean_w <- full$penetrance_lqt1
covariates <- c("blast_pssm", "revel_score", "cardiacboost", "hm_peak", "lqt1_dist")

# delta to 1 on 1/24/2021
delta <- 10
count <- 0
tmp <- full
options(warn = -1)

while(delta > 1 & count < 25){ # delta = 5 is roughly a change of 0.5%
  print(paste(delta, count))
  count <- count + 1
  alpha_f <- NULL
  beta_f <- NULL
  
  for(i in 1:nrow(tmp)){
  newdata = data.frame(var=tmp[i,"var"])
  newdata[covariates] <- tmp[i,covariates]
  model <- regression("p_mean_w", covariates, 
                      colnames(newdata)[colSums(is.na(newdata))>0], tmp)
  mean_f <- predict(model, newdata, type = "response")
  variance_f <- (predict(model, newdata,se.fit = T, type = "response")$se.fit)^2
  alpha <- solab(mean_f,variance_f)[1]
  beta <- solab(mean_f,variance_f)[2]
  tmp[i,"prior_mean_w"] <- mean_f
  if(alpha<0.01 | beta<0.01){
    alpha_f[i]=alpha0
    beta_f[i]=beta0
  }else{
    alpha_f[i]=alpha
    beta_f[i]=beta
  }
  }
  new_mean <- (alpha_f + tmp$lqt1)/(alpha_f + beta_f + tmp$total_carriers)
  full$prior_mean_w <- tmp$prior_mean_w
  delta <- sum(abs(new_mean-tmp$p_mean_w))
  tmp$p_mean_w <- new_mean
  full$p_mean_w <- tmp$p_mean_w
  print(delta)
}

full2 <- full[full$weight == 0, ]
brier_scores[j] <- brier_calc(full2$lqt1_penetranceBayesian, full2$prior_mean_w, length(full2$lqt1_penetranceBayesian))
print(brier_scores)
}

```

## Full Brier scores for other covariates

```{r}

# Data for main text 
rm(d)
load('lit_plus_cohort_checkpoint_eval.RData')
d <- d[d$total_carriers >0, ]

# Prior - 0.028

d <- d[!is.na(d$lqt1_penetranceBayesian), ]
d_prior <- d[!is.na(d$prior_mean_w), ]
brier_calc(d_prior$lqt1_penetranceBayesian, d_prior$prior_mean_w, length(d_prior$var))

# Posterior - 0.010
d_posterior <- d[!is.na(d$p_mean_w), ]
brier_calc(d_posterior$lqt1_penetranceBayesian, d_posterior$p_mean_w, length(d_posterior$var))

# Revel - 0.17
d_revel <- d[!is.na(d$revel_score), ]
brier_calc(d_revel$lqt1_penetranceBayesian, d_revel$revel_score, length(d_revel$var))

# LQT1_dist - 0.062
d_lqt1_dist <- d[!is.na(d$lqt1_dist), ]
brier_calc(d_lqt1_dist$lqt1_penetranceBayesian, d_lqt1_dist$lqt1_dist, length(d_lqt1_dist$var))

# Cardiac Boost - 0.19
d_cardiacboost <- d[!is.na(d$cardiacboost), ]
brier_calc(d_cardiacboost$lqt1_penetranceBayesian, d_cardiacboost$cardiacboost, length(d_cardiacboost$var))

# Polyphen - 0.22
d_pph2_prob <- d[!is.na(d$pph2_prob), ]
brier_calc(d_pph2_prob$lqt1_penetranceBayesian, d_pph2_prob$pph2_prob, length(d_cardiacboost$var))


```

# Coverage Plots
In addition to an estimate of penetrance, we also provide an uncertainty of that estimate. For example, 7 of 10 affected heterozygotes has the same point estimate as 700/1000 affected heterozygotes, but there is much less uncertainty in the latter. We therefore calibrate this uncertainty in terms of number of affected heterozygotes in our estimate, using th strategy outlined below. 

## Bootstrap and get the coverage rate 
(1) Use the observed diagnosis probability from as the TRUE diagnosis probability, generate n binomial observations

(2) Use the final EM algorithm posterior as the prior for Beta-Binomial, incorporate data from step (1), generate the posterior distribution, and get 95% credible interval.

(3) Check whether the interval cover the true diagnosis probability from Step 1.

(4) Repeat Step 1 to Step 3 N times to get the coverage rate.  

### Bootstrap function 

```{r}

BootsCoverage <- function(var,n=100,N=1000,true){
  
  # var: variant name
  # n: number of subjects in the new data
  # N: number of Bootstrap

  # extract the "true" diagnosis probability
  true.p <- d[d$var==var,true]
  
  # generate binomial data 
  event <- rbinom(N,n,true.p)

  # get the posterior credible interval
  alpha <- d$alpha[which(d$var==var)] 
  beta <- d$beta[which(d$var==var)] 

  new.alpha <- alpha + event
  new.beta <- beta + n - event

  lb <- qbeta(0.025,new.alpha,new.beta)
  ub <- qbeta(0.975,new.alpha,new.beta)

  # change lb to floor of nearest 0.1
  lb <- floor(lb*20)/20
  ub <- ceiling(ub*20)/20
  
  return(sum(lb < true.p & ub > true.p)/N) 
}

``` 

## Plot coverage
Observed diagnosis probability as the "true" diagnosis probability. The coverage plot where observed diagnosis probability is the "true" diagnosis probability and one hundred new observations are added is shown below. 


```{r}

# Data for Supplemental Figure 3

d$alpha <- NULL
d$beta <- NULL

# solve for alpha and beta in Beta distribution
solab <- function(mean, variance){
  alpha <- (mean^2 * (1-mean) - variance * mean)/variance
  beta <- alpha * (1 / mean - 1)
  return(c(alpha,beta))
}

nu = 10 # Change tuning parameter for each panel within Supplementary Figure 2 
prior_mean <- d$prior_mean_w
variance <- prior_mean*(1-prior_mean)
variance <- variance /(1 + nu)
ind_a <- seq(1, length(variance),1)
ind_b <- seq(length(variance)+1, length(variance)*2,1)
alpha <- solab(prior_mean,variance)[ind_a]
beta <- solab(prior_mean,variance)[ind_b]

new_mean <- (alpha + d$lqt1)/(alpha + beta + d$total_carriers)
d$p_mean_w <- new_mean

d$alpha <- alpha
d$beta <- beta

d<-d[d$total_carriers>0,]
d$var<-factor(d$var)

########## when n = 100 ##########

results <- sapply(d$var,function(x) BootsCoverage(x, n=100, true="penetrance_lqt1") )

carriers.size <- ifelse(d$total_carriers <= 10,1,ifelse(d$total_carriers <= 100,2,ifelse(d$total_carriers <= 1000,3,ifelse(d$total_carriers <= 10^4,4,5) )))

new.data <- data.frame(Penetrance=d$penetrance_lqt1, Coverage=results, Number=log10(d$total_carriers))

ggplot(data=new.data,aes(x=Penetrance,y=Coverage))+geom_point(aes(size=Number,color=Number),shape=20)+geom_hline(yintercept = 0.95,color="red")+scale_x_continuous(limits=c(0,1))+scale_y_continuous(limits=c(0,1))+labs(x=" True penetrance under simulation", y="Coverage rate", size=TeX("$\\log_{10}$(Total number of carriers)"),color=TeX("$\\log_{10}$(Total number of carriers)"))+
  theme(legend.position = "bottom",legend.box = 'vertical',legend.justification = 'left',legend.box.just = 'left',legend.title = element_text(size=8))+scale_colour_gradient(low = "dodgerblue", high = "black")
```


